{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import math\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemm = StemmerFactory()\n",
    "stemmer = stemm.create_stemmer()\n",
    "stop = StopWordRemoverFactory()\n",
    "stopwords = stop.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:\\Programming\\Python\\Text Mining\\document-classifier-naive-bayes\\dataset.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get komentar per dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = data.loc[:,'Komentar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Menghapus berbagai simbol pada kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(desc):\n",
    "    desc[i] = (\n",
    "        val.replace(\";\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\".\", \" \") # Tambah spasi biar kata yang dipisah sama titik bisa dipisah\n",
    "        .replace(\"?\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"/\", \" \")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi_kata(sentences):\n",
    "    tokenizing = []\n",
    "    for val in sentences:\n",
    "        for value in val.split():\n",
    "            tokenizing.append(value)\n",
    "\n",
    "    return tokenizing\n",
    "\n",
    "def memfilter(doc):\n",
    "    filtering = []\n",
    "    for i in doc:\n",
    "        if i not in stopwords:\n",
    "            filtering.append(i)\n",
    "\n",
    "    filtering = list((filtering))\n",
    "    return filtering\n",
    "\n",
    "def menstem(doc1):\n",
    "    stemming = []\n",
    "    for i in memfilter(doc1):\n",
    "        stemming.append(stemmer.stem(i))\n",
    "\n",
    "    stemming = list((stemming))\n",
    "    return stemming\n",
    "\n",
    "def getFreq(dicti, word):\n",
    "    for word in word:\n",
    "        if word not in dicti:\n",
    "            continue\n",
    "        dicti[word] += 1\n",
    "\n",
    "    return dicti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenized = tokenisasi_kata(desc)\n",
    "print(\"Tokenizing :\", word_tokenized, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"filtering : \", memfilter(word_tokenized), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stemming + filter stop words. Sisa kata unik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = set(menstem((word_tokenized)))\n",
    "print(len(wordset), \"\\nstemming : \", wordset, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mencari TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mengumpulkan seluruh kata dalam kalimat pada list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "freqs = []\n",
    "\n",
    "for i, val in enumerate(desc):\n",
    "    sentences.append(val)\n",
    "\n",
    "for i, val in enumerate(sentences):\n",
    "    words.append(sentences[i].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Menghitung frekuensi kata unik pada dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(sentences):    \n",
    "    freqs.append(getFreq(dict.fromkeys(wordset, 0), menstem(words[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF\")\n",
    "for i, val in enumerate(freqs):\n",
    "    tf = pd.DataFrame([freqs[i\n",
    "    ]])\n",
    "tf = pd.DataFrame(freqs)\n",
    "print(tf)\n",
    "print('\\n')\n",
    "# print(tf.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Menghitung nilai |V| dan jumlah seluruh kata pada kategori positif dan negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = len(wordset)\n",
    "# desc1 = data.loc[:,['Komentar','Hasil Akhir']]\n",
    "# print(desc1)\n",
    "#pre processing kata dalam kategori positif\n",
    "dataset = data.loc[:, [\"Komentar\",\"Hasil Akhir\"]]\n",
    "df = pd.DataFrame(dataset)\n",
    "dataset_positif = df.loc[df[\"Hasil Akhir\"] == \"Positif\"][\"Komentar\"].values.tolist()\n",
    "\n",
    "for i, val in enumerate(dataset_positif):\n",
    "    dataset_positif[i] = (\n",
    "        val.replace(\";\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "        .replace(\"?\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"/\", \" \")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "\n",
    "word_tokenized_positif = tokenisasi_kata(dataset_positif)\n",
    "filtered_word_positif = memfilter(word_tokenized_positif)\n",
    "wordset_positif = menstem((word_tokenized_positif))\n",
    "count_positif = len(wordset_positif)\n",
    "print(count_positif)\n",
    "#pre processing kata dalam kategori negatif\n",
    "dataset_negatif = df.loc[df[\"Hasil Akhir\"] == \"Negatif\"][\"Komentar\"].values.tolist()\n",
    "for i, val in enumerate(dataset_negatif):\n",
    "    dataset_negatif[i] = (\n",
    "        val.replace(\";\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "        .replace(\"?\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"/\", \" \")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "\n",
    "word_tokenized_negatif = tokenisasi_kata(dataset_negatif)\n",
    "filtered_word_negatif = memfilter(word_tokenized_negatif)\n",
    "wordset_negatif = menstem((word_tokenized_negatif))\n",
    "count_negatif = len(wordset_negatif)\n",
    "print(count_negatif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}